[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sarah Hannes",
    "section": "",
    "text": "LinkedIn\n  \n  \n    \n     Github\n  \n  \n    \n     Download CV\n  \n\n      \n\n\nPython Pandas Tensorflow Keras Scikit-Learn\nLightGBM SQL MongoDB \n\n\n\nBottomless | Aug 2022 - Present\nData Scientist\nDHL | May 2021 ‚Äì Oct 2021\nData Scientist Intern\nAccenture | Jul 2019 ‚Äì Oct 2020\nQuality Auditor\n\n\n\nUniversity of Technology, Malaysia (UTM) | March 2022\nMaster of Science (Data Science)\nUniversity of Science, Malaysia (USM) | July 2016\nBachelor of Applied Biology, Hons (Biotechnology)"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Sarah Hannes",
    "section": "",
    "text": "Bottomless | Aug 2022 - Present\nData Scientist\nDHL | May 2021 ‚Äì Oct 2021\nData Scientist Intern\nAccenture | Jul 2019 ‚Äì Oct 2020\nQuality Auditor"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Sarah Hannes",
    "section": "",
    "text": "University of Technology, Malaysia (UTM) | March 2022\nMaster of Science (Data Science)\nUniversity of Science, Malaysia (USM) | July 2016\nBachelor of Applied Biology, Hons (Biotechnology)"
  },
  {
    "objectID": "blog/visualizing_arrays.html",
    "href": "blog/visualizing_arrays.html",
    "title": "Visualizing Arrays using Excel",
    "section": "",
    "text": "Visualizing simple operations across arrays\n\n\nimport numpy as np\nimport torch\nfrom torch import tensor\n\nIn deep learning world, we are always working with arrays and tensors. They are data structure in which we store data for our model to train on. Both are multi-dimensional data structure and have similar functionality, but tensors had more restrictions than arrays.\nTensor must: - use a single basic numerical type for all components in the array. - can not be jagged. It is always regularly shaped multi-dimensional rectangular structure.\nI always had trouble trying to understand what exactly happen when we perform operations on any data structure that has higher dimension than a list (dimension of 1).\nSo, let‚Äôs use Excel to visualize some basic operations on arrays (or tensors)! Say we have 2 tensors, a and b. Both of dimension of 3.\n\n\n\nimage-2.png\n\n\n\na = tensor([[[1,2,3,4],\n            [40,50,60,70],\n            [7,8,9,10]], \n           \n           [[11,12,13,14],\n           [140,150,160,170],\n           [17,18,19,20]]])\n\n\nb = tensor([[[1,1,1,1],\n            [2,2,2,2],\n            [3,3,3,3]],\n           \n           [[1,1,1,1],\n           [2,2,2,2],\n           [3,3,3,3]]])\n\nprint('a shape:\\t', a.shape)\nprint('b shape:\\t', b.shape)\n\na shape:     torch.Size([2, 3, 4])\nb shape:     torch.Size([2, 3, 4])\n\n\n\nElement wise operation\n\n\n\nimage-2.png\n\n\nIf we subtract tensor a - tensor b, we are doing elementwise operation because both tensors a and b are of the same shape.\n\na_b = a - b\nprint('a_b shape:\\t', a_b.shape)\na_b\n\na_b shape:   torch.Size([2, 3, 4])\n\n\ntensor([[[  0,   1,   2,   3],\n         [ 38,  48,  58,  68],\n         [  4,   5,   6,   7]],\n\n        [[ 10,  11,  12,  13],\n         [138, 148, 158, 168],\n         [ 14,  15,  16,  17]]])\n\n\n\n\nSum across tensor\n\n\n\nimage.png\n\n\nIf we sum across the whole tensor, we will get a scalar value (dimension 0). Scalar value is another name for a number.\n\nprint(a_b.sum().shape)\na_b.sum()\n\ntorch.Size([])\n\n\ntensor(960)\n\n\n\n\nSum across axis\nNow, let‚Äôs do sum operation across axis. Axis could be sample, row or column. Notice that our tensor a_b had shape of (2,3,4). This means it is made up of 2 samples (E23:H25 cells is a sample, E27:H29 cells is the other sample), each sample has 3 rows and 4 columns. So from shape information, we know (total samples, total rows in each sample, total columns in each sample).\nIn code, the shape information is often represented as tuple data structure. Which means we can also do negative indexing, ie: - shape at index -2 == column information - shape at index -1 == row information - shape at index 0 == sample information\nSum across any axis would collapse the specified axis. What do I mean by this?\nOriginally, our a_b tensor is of shape (2, 3, 4). If we add all rows, keeping everything else the same. Our resulting shape would be (2, 3, 4) ‚Äì&gt; (2, 1, 4)\n\n\n\nimage.png\n\n\n\nprint(a_b.sum(-2, keepdim=True).shape)\na_b.sum(-2, keepdim=True)\n\ntorch.Size([2, 1, 4])\n\n\ntensor([[[ 42,  54,  66,  78]],\n\n        [[162, 174, 186, 198]]])\n\n\nYup, this below is the same. The only difference here is that we use index here. Above, we use negative indexing style. The same thing, just different way of writing, don‚Äôt let it confuse you.\n\n\nprint(a_b.sum(1, keepdim=True).shape)\na_b.sum(1, keepdim=True)\n\ntorch.Size([2, 1, 4])\n\n\ntensor([[[ 42,  54,  66,  78]],\n\n        [[162, 174, 186, 198]]])\n\n\n\n\n\nimage.png\n\n\nSimilarly, if we add all columns, keeping everything the same. Then our resulting shape would be (2, 3, 4) ‚Äì&gt; (2, 3, 1)\n\nprint(a_b.sum(-1, keepdim=True).shape)\na_b.sum(-1, keepdim=True)\n\ntorch.Size([2, 3, 1])\n\n\ntensor([[[  6],\n         [212],\n         [ 22]],\n\n        [[ 46],\n         [612],\n         [ 62]]])\n\n\n\nprint(a_b.sum(2, keepdim=True).shape)\na_b.sum(2, keepdim=True)\n\ntorch.Size([2, 3, 1])\n\n\ntensor([[[  6],\n         [212],\n         [ 22]],\n\n        [[ 46],\n         [612],\n         [ 62]]])\n\n\n\n\n\nimage.png\n\n\nNow, if we add all samples, keeping everything else the same. Our result would be from shape (2, 3, 4) ‚Äì&gt; (1, 3, 4).\n\nprint(a_b.sum(0, keepdim=True).shape)\na_b.sum(0, keepdim=True)\n\ntorch.Size([1, 3, 4])\n\n\ntensor([[[ 10,  12,  14,  16],\n         [176, 196, 216, 236],\n         [ 18,  20,  22,  24]]])\n\n\n\nprint(a_b.sum(-3, keepdim=True).shape)\na_b.sum(-3, keepdim=True)\n\ntorch.Size([1, 3, 4])\n\n\ntensor([[[ 10,  12,  14,  16],\n         [176, 196, 216, 236],\n         [ 18,  20,  22,  24]]])\n\n\n\n\nSum across multiple axis\n\n\n\nimage.png\n\n\nWe can also do sum across multiple axis. Remember that our original tensor is of shape (2, 3, 4), if we do sum operation across both rows and columns, then we would be collapsing both of these axis, keeping everything else the same. So the resulting tensor would be from shape (2, 3, 4) ‚Äì&gt; (2, 1, 1).\n\nprint(a_b.sum((-1,-2), keepdim=True).shape)\na_b.sum((-1,-2), keepdim=True)\n\ntorch.Size([2, 1, 1])\n\n\ntensor([[[240]],\n\n        [[720]]])\n\n\nAlso notice that the order in which we specify the axes does not matter. .sum((-1, -2)) is the same as .sum((-2, -1)).\n\nprint(a_b.sum((-2,-1), keepdim=True).shape)\na_b.sum((-2,-1), keepdim=True)\n\ntorch.Size([2, 1, 1])\n\n\ntensor([[[240]],\n\n        [[720]]])\n\n\n\n\n\nimage.png\n\n\nNow we sum all samples and rows, keeping everything else the same. The resulting tensor would be from (2, 3, 4) ‚Äì&gt; (1, 1, 4).\n\nprint(a_b.sum((0,1), keepdim=True).shape)\na_b.sum((0,1), keepdim=True)\n\ntorch.Size([1, 1, 4])\n\n\ntensor([[[204, 228, 252, 276]]])\n\n\n\n\n\nimage.png\n\n\nLastly, if we sum all samples and columns, then the resulting tensor would be from (2, 3, 4) ‚Äì&gt; (1, 3, 1).\nNotice that for resulting value 52, our excel formula is E23:H23 + E27:H27 ‚âà we add everything from first row in sample 1, plus everything from first row in sample 2.\n\nprint(a_b.sum((0,-1), keepdim=True).shape)\na_b.sum((0,-1), keepdim=True)\n\ntorch.Size([1, 3, 1])\n\n\ntensor([[[ 52],\n         [824],\n         [ 84]]])\n\n\nHopefully, now it is easy to notice that any axis we choose to perform operations on would be collapsed to 1. Which means, when we collapsed all axes, it is equivalent to sum across everything in the tensor. ie in code, this is:\n\nprint(a_b.sum((0,1,2)).shape)\na_b.sum((0,1,2))\n\ntorch.Size([])\n\n\ntensor(960)\n\n\nwhich is the same as:\n\nprint(a_b.sum().shape)\na_b.sum()\n\ntorch.Size([])\n\n\ntensor(960)\n\n\nAll above operations can be also performed using numpy arrays. For example, the equivalent summing across samples and rows in numpy is:\n\nprint(np.array(a_b).sum((0,-1), keepdims=True).shape)\nnp.array(a_b).sum((0,-1), keepdims=True)\n\n(1, 3, 1)\n\n\narray([[[ 52],\n        [824],\n        [ 84]]])\n\n\nVisualizing these tensors and formulas on excel makes it easier to for me to see what happen in the background. Hopefully, it is helpful to you too."
  },
  {
    "objectID": "til/190624_howtodocker.html",
    "href": "til/190624_howtodocker.html",
    "title": "How to build üê≥ Docker Image & run Container",
    "section": "",
    "text": "Step by step guide on how to build docker image on VSCode and run docker container on Docker Desktop App\n\nIt is seldom that I needed to use docker to validate pre-production code, but when I do, I find myself always having to retrace my steps trying to learn how to use docker again. So here is step by step guide to remind myself:\n\n1. Build docker image\n\nOpen VSCode window, choose ‚ÄúOpen Folder‚Äù.\nChoose the directory where we store the pre-production code we want to test. Eg ‚Äúmyfolder‚Äù here.\n\n\n.\n‚îî‚îÄ‚îÄ myfolder/\n    ‚îú‚îÄ‚îÄ newmodel_prediction.py\n    ‚îú‚îÄ‚îÄ requirements.txt\n    ‚îú‚îÄ‚îÄ Dockerfile\n    ‚îú‚îÄ‚îÄ .gitignore\n    ‚îî‚îÄ‚îÄ .dockerignore\n\n\nIf not already exist, add .dockerignore. If the file we want to ignore is not in ‚Äò./myfolder‚Äô directory, we need to include full filepath.\n\n\n# .dockerignore\nDockerfile\nbin\ninclude\nlib\nvenv\n.env\n\n\nIf not already exist, create .gitignore. Fill in any filenames that you want to exclude. If the file we want to ignore is not in ‚Äò./myfolder‚Äô directory, we need to include full filepath.\n\n\n# .gitignore\n.env\nbin\nlib\ninclude\nshare\n.ipynb_checkpoints\n\n\nIf not already exist, create requirements.txt. We can automatically create one using terminal. In terminal:\n\n\n# make sure we have pipreqs installed, if not install using:\n!pip install pipreqs\n\n# create requirements.txt in the parent directory\n!pipreqs .\n\n\nCreate Dockerfile. Currently, below is the minimal working dockerfile we can use to test any pre-production code. The last line CMD [\"python3.8\", \"./newmodel_prediction.py\"] means as soon as the container is running, run python 3.8 ./newmodel_prediction.py.\n\n\n# Dockerfile\n\n# enter what specific python image we want to work with\nFROM python:3.8.16\n\nRUN apt-get install libgomp1\n\nWORKDIR /usr/local/bin\nADD . ./\n\nRUN python -m pip install -r requirements.txt\n\n# what command we want to run as soon as the container is running\nCMD [\"python3.8\", \"./newmodel_prediction.py\"]\n\n\nOpen up docker desktop apps.\nBuild docker image. The easiest way to create an image in VSCode:\n\nOpen up Dockerfile\nIn Dockerfile, right-click and select ‚ÄúBuild Image‚Ä¶‚Äù \nLook notice the ‚Äúmyfolder:1‚Äù in the command pallete. ‚Äúmyfolder‚Äù is there because it is the name of our current directory, this will become the default image name. ‚Äú1‚Äù here is the default image tag. We can change both the image name and the tag if we wish, or just press enter to use the default. \nIf there is no error, your terminal should look similar like so. \nObserve that we have newly created image in our docker desktop app. \n\n\n\n\n2. Create docker container & run\n\nClick on the newly created image. \nSet container name (optional). Add any environment variables if needed. The environment variables you included here will be used to run the container. (See docker documentation for environment variables precedence) \nYou will be directed to container tab. Notice the container name ‚Äútestrun1‚Äù, image name and tag ‚Äúmyfolder:1‚Äù.You can confirm that the container is currently running when the play button ‚ñ∂Ô∏è on the right hand corner is grayed out. You can stop the container by clicking on the stop button ‚èπ. Since we created our ended our Dockerfile with CMD [\"python3.8\", \"./newmodel_prediction.py\"], this means it will directly run the specified file. However, we could also manually run anything through the terminal should we wish, like so. \n\nNote: Docker containers and images take up space. You might want to delete them if unused."
  },
  {
    "objectID": "til/190624_lesson0.html",
    "href": "til/190624_lesson0.html",
    "title": "Fastai Lesson0 Key Insights",
    "section": "",
    "text": "Notes I made while listening to Fastai Lesson0\n\n\nFinish the course.\n\n‚ÄúTenacity‚Äù is a choice\n‚ÄúTenacity‚Äù is not about ignoring the bumps but it is keeping going after the bumps\n\nRadek‚Äôs book Meta Learning: How to Learn Deep Learning And Thrive In The Digital World:\n\n4-legged table that will help you do your deep learning experiments more effectively and efficiently:\n\nCode concepts: knowing the basic ideas around the code\nEditor: knowing your tools\ngit & github: knowing how to save your work, how to pull people‚Äôs work\nssh/ Linux: knowing how to access a server and do stuff with it\n\n\nThere‚Äôs nothing to be ashamed of if you‚Äôve never used git or ssh or whatever, they are just tools that most people have to figure out in the journey.\nRecommended MOOC to get started on programming:\n\nHarvard CS50\nThe Missing Semester of Your CS Education\n\nFocus on making 1 project great and polishing it off and finishing it.\n\nThe project doesn‚Äôt have to be a pioneer\n\nWrite a blog.\n\nGood place to start: write down a talk/video that you like. Doing this helps the person giving the talk reaching people using a second medium, and help others learning about the talk in writings (a lot of poeple prefer reading than listening).\n\nCreate a good validation set.\n\nBlog post by Rachel Thomas\n\nAlways start with a simple baseline.\n\nA baseline: the simplest model you can that you know can solve the problem so simply that you can‚Äôt make any mistakes. Below are some example baselines:\n\nTaking the average of your data as the ypred for each ‚Äògroup‚Äô\n\nIn deep learning, it is much harder to see that you are wrong, often it is just going to be half percent less accurate etc. For example, it could be that you unknowingly fed in images are upside down, and trained a model that can only recognize upside down images.\n\nSuccessful machine learning projects:\n\nCreate the simplest possible solution that gets something all the way from end-to-end.\nGradually making it slightly better.\n\nDo Kaggle competition as your project.\n\nMindset: To do your best, instead of trying to win on your first competition.\n\nImportant ingredients on finding a job:\n\nPortfolio: blog posts, github projects, community engagement"
  },
  {
    "objectID": "til/130624_fastai_AttributeError.html",
    "href": "til/130624_fastai_AttributeError.html",
    "title": "TabularPandas AttributeError: classes",
    "section": "",
    "text": "Specify procs TabularPandas(..., procs = [Categorify]) when you have categorical columns\n\n\n\n\n\n\n\nNote\n\n\n\nThis post was written using: - pandas: 2.2.2 - fastai: 2.7.15\n\n\n\nimport pandas as pd\nfrom fastai.tabular.all import *\n\n\n# looking at unique values in each columns to split categorical / continuous features\nfor k in df.keys():\n    print(f\"Column {k}:\\n{Counter(df[k])}\")\n    print()\n\nColumn a:\nCounter({2.0: 8435, 1.0: 2580, 3.0: 2011})\n\nColumn b:\nCounter({12.0: 9853, 32.0: 2911, 80.0: 87, 16.0: 72, 11.0: 47, 8.0: 28, 10.0: 25, 40.0: 3})\n\nColumn c:\nCounter({0: 1579, 5: 1517, 6: 1271, 7: 1213, 4: 1211, 8: 1081, 3: 898, 9: 831, 10: 682, 2: 608, 1: 342, 11: 278, 14: 216, 12: 206, 16: 184, 13: 165, 15: 145, 17: 131, 18: 94, 19: 88, 20: 66, 21: 57, 23: 54, 29: 24, 26: 18, 24: 16, 27: 16, 22: 12, 30: 12, 25: 7, 28: 4})\n\nColumn d:\nCounter({15.0: 7521, -1.0: 1433, 8.0: 374, 0.0: 372, 7.0: 365, 9.0: 334, 10.0: 319, 6.0: 316, 5.0: 272, 11.0: 264, 4.0: 241, 12.0: 220, 3.0: 217, 13.0: 205, 14.0: 176, 2.0: 170, 1.0: 164, -3.0: 14, -2.0: 4, -4.0: 3, -27.0: 3, -5.0: 3, -22.0: 2, -26.0: 2, -30.0: 2, -16.0: 2, -7.0: 2, -17.0: 2, -21.0: 1, -23.0: 1, -25.0: 1, -28.0: 1, -31.0: 1, -32.0: 1, -53.0: 1, -56.0: 1, -57.0: 1, -58.0: 1, -59.0: 1, -88.0: 1, -93.0: 1, -98.0: 1, -38.0: 1, -8.0: 1, -11.0: 1, -14.0: 1, -18.0: 1, -9.0: 1, -10.0: 1, -43.0: 1, -49.0: 1, -6.0: 1})\n\nColumn e:\nCounter({4.1: 1285, 3.4: 912, 3.3: 905, 4.0: 884, 4.2: 812, 3.5: 797, 3.6: 713, 3.7: 708, 3.2: 666, 3.9: 640, 3.8: 628, 4.3: 512, 4.4: 359, 2.1: 351, 4.5: 294, 3.1: 276, 2.2: 269, 4.6: 216, 2.3: 185, 4.7: 175, 2.4: 145, 4.8: 142, 4.9: 135, 5.0: 130, 5.1: 90, 2.5: 88, 5.2: 72, 2.6: 66, 2.8: 64, 2.7: 64, 5.3: 58, 3.0: 54, 2.9: 50, 2.0: 38, 5.4: 34, 5.5: 28, 6.3: 22, 6.6: 21, 5.7: 16, 6.2: 16, 5.6: 16, 5.8: 15, 6.7: 14, 6.8: 12, 6.1: 12, 6.5: 9, 5.9: 8, 6.4: 6, 6.0: 4, 7.2: 4, 6.9: 3, 7.0: 1, 7.5: 1, 7.1: 1})\n\nColumn f:\nCounter({0.7: 1475, 0.6: 1472, 0.8: 1444, 1.0: 1385, 0.9: 1335, 0.5: 1249, 0.4: 1014, 1.1: 974, 0.3: 708, 0.0: 482, 0.2: 423, 0.1: 294, 1.2: 241, 1.3: 119, 1.4: 81, 1.5: 56, 1.6: 37, 1.8: 33, 1.9: 25, -0.1: 23, 1.7: 22, -1.1: 19, 2.3: 15, 2.4: 12, 2.1: 9, -1.3: 9, -0.6: 8, -0.9: 8, -0.7: 7, 2.0: 7, -0.8: 5, -1.9: 5, -1.2: 4, 2.2: 4, -0.2: 4, 2.8: 3, 2.6: 2, 2.5: 2, -0.3: 2, -0.5: 2, 5.1: 1, -1.0: 1, 2.7: 1, 3.0: 1, -1.8: 1, -1.6: 1, -4.5: 1})\n\nColumn label:\nCounter({0.0: 11000, 1.0: 2026})\n\n\n\n\n# define categorical and continuous features\ncat_names = ['a', 'b']\ny_names = 'label'\ncont_names = [c for c in df.keys() if c not in cat_names+[y_names]]\n\n\nprint('cat_names:',cat_names)\nprint('cont_names:',cont_names)\nprint('y_names:',y_names)\n\ncat_names: ['a', 'b']\ncont_names: ['c', 'd', 'e', 'f']\ny_names: label\n\n\n\n# split into train and test\nval_index = list(df.sample(frac=0.2, random_state=0).index) # 20% from total df\ntrain_index = list(df[~df.index.isin(val_index)].index)\n\nassert (len([i for i in train_index if i in set(val_index)])==0 \n        and len([i for i in val_index if i in set(train_index)])==0), 'train and val set are overlapping!'\n\nprint('train set len', len(train_index))\nprint('val set len', len(val_index))\n\ntrain set len 10421\nval set len 2605\n\n\n\nError Example\n\n# oh no, can't train!\n\ndl = TabularPandas(df, \n                   cat_names=cat_names, \n                   cont_names=cont_names, \n                   y_names=y_names,\n                   y_block = CategoryBlock(vocab=df[y_names]), \n                   splits=(train_index, val_index))\n\ndls = dl.dataloaders(bs=64)\nprint(dls.show_batch())\nlearn = tabular_learner(dls, metrics=[accuracy])\nlearn.fit_one_cycle(3)\n\n\n\n\n\na\nb\nc\nd\ne\nf\nlabel\n\n\n\n\n0\n1.0\n12.0\n7.0\n9.0\n3.9\n0.6\n0.0\n\n\n1\n2.0\n12.0\n3.0\n15.0\n4.1\n0.3\n0.0\n\n\n2\n2.0\n12.0\n4.0\n-1.0\n4.0\n0.7\n0.0\n\n\n3\n2.0\n12.0\n11.0\n15.0\n4.1\n1.4\n0.0\n\n\n4\n2.0\n12.0\n4.0\n12.0\n4.2\n0.6\n0.0\n\n\n5\n2.0\n32.0\n14.0\n6.0\n5.2\n0.2\n0.0\n\n\n6\n1.0\n12.0\n4.0\n9.0\n3.2\n0.3\n1.0\n\n\n7\n3.0\n32.0\n5.0\n15.0\n3.5\n0.7\n0.0\n\n\n8\n2.0\n12.0\n3.0\n14.0\n2.6\n0.5\n0.0\n\n\n9\n3.0\n12.0\n0.0\n-2.0\n4.1\n-0.0\n0.0\n\n\n\n\n\nNone\n\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[43], line 10\n      8 dls = dl.dataloaders(bs=64)\n      9 print(dls.show_batch())\n---&gt; 10 learn = tabular_learner(dls, metrics=[accuracy])\n     11 learn.fit_one_cycle(3)\n\nFile /opt/homebrew/Caskroom/miniforge/base/envs/fastai/lib/python3.11/site-packages/fastai/tabular/learner.py:42, in tabular_learner(dls, layers, emb_szs, config, n_out, y_range, **kwargs)\n     40 if layers is None: layers = [200,100]\n     41 to = dls.train_ds\n---&gt; 42 emb_szs = get_emb_sz(dls.train_ds, {} if emb_szs is None else emb_szs)\n     43 if n_out is None: n_out = get_c(dls)\n     44 assert n_out, \"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`\"\n\nFile /opt/homebrew/Caskroom/miniforge/base/envs/fastai/lib/python3.11/site-packages/fastai/tabular/model.py:32, in get_emb_sz(to, sz_dict)\n     27 def get_emb_sz(\n     28     to:Tabular|TabularPandas, \n     29     sz_dict:dict=None # Dictionary of {'class_name' : size, ...} to override default `emb_sz_rule` \n     30 ) -&gt; list: # List of embedding sizes for each category\n     31     \"Get embedding size for each cat_name in `Tabular` or `TabularPandas`, or populate embedding size manually using sz_dict\"\n---&gt; 32     return [_one_emb_sz(to.classes, n, sz_dict) for n in to.cat_names]\n\nFile /opt/homebrew/Caskroom/miniforge/base/envs/fastai/lib/python3.11/site-packages/fastai/tabular/model.py:32, in &lt;listcomp&gt;(.0)\n     27 def get_emb_sz(\n     28     to:Tabular|TabularPandas, \n     29     sz_dict:dict=None # Dictionary of {'class_name' : size, ...} to override default `emb_sz_rule` \n     30 ) -&gt; list: # List of embedding sizes for each category\n     31     \"Get embedding size for each cat_name in `Tabular` or `TabularPandas`, or populate embedding size manually using sz_dict\"\n---&gt; 32     return [_one_emb_sz(to.classes, n, sz_dict) for n in to.cat_names]\n\nFile /opt/homebrew/Caskroom/miniforge/base/envs/fastai/lib/python3.11/site-packages/fastcore/basics.py:507, in GetAttr.__getattr__(self, k)\n    505 if self._component_attr_filter(k):\n    506     attr = getattr(self,self._default,None)\n--&gt; 507     if attr is not None: return getattr(attr,k)\n    508 raise AttributeError(k)\n\nFile /opt/homebrew/Caskroom/miniforge/base/envs/fastai/lib/python3.11/site-packages/fastcore/transform.py:212, in Pipeline.__getattr__(self, k)\n--&gt; 212 def __getattr__(self,k): return gather_attrs(self, k, 'fs')\n\nFile /opt/homebrew/Caskroom/miniforge/base/envs/fastai/lib/python3.11/site-packages/fastcore/transform.py:173, in gather_attrs(o, k, nm)\n    171 att = getattr(o,nm)\n    172 res = [t for t in att.attrgot(k) if t is not None]\n--&gt; 173 if not res: raise AttributeError(k)\n    174 return res[0] if len(res)==1 else L(res)\n\nAttributeError: classes\n\n\n\nHow to fix this?\nI actually went down the rabbit hole and provided the emb_szs manually as mentioned in the source code hinted by error message above, but there is actually an easier way ‚Äì just add procs=[Categorify] when initializing TabularPandas.\n\nIn the source code, emb_szs is expected to be {'class_name' : size, ...}. So for example if column a is a categorical column in our df, then emb_szs = {'a': len(unique value in column 'a')}.\n\n\n\nWorking Example\n\n# now we can train\n\ndl = TabularPandas(df, \n                   cat_names=cat_names, \n                   cont_names=cont_names, \n                   y_names=y_names,\n                   y_block = CategoryBlock(vocab=df[y_names]), \n                   splits=(train_index, val_index),\n                   procs=[Categorify])  # &lt;------ add procs!\n\ndls = dl.dataloaders(bs=64)\nprint(dls.show_batch())\nlearn = tabular_learner(dls, metrics=[accuracy])\nlearn.fit_one_cycle(3)\n\n\n\n\n\na\nb\nc\nd\ne\nf\nlabel\n\n\n\n\n0\n2.0\n32.0\n15.0\n15.0\n3.6\n0.8\n0.0\n\n\n1\n3.0\n32.0\n9.0\n15.0\n4.5\n0.7\n0.0\n\n\n2\n2.0\n32.0\n9.0\n14.0\n3.9\n0.5\n1.0\n\n\n3\n2.0\n12.0\n6.0\n-1.0\n4.0\n1.0\n0.0\n\n\n4\n2.0\n12.0\n8.0\n15.0\n3.7\n0.9\n0.0\n\n\n5\n2.0\n12.0\n4.0\n15.0\n3.7\n0.7\n1.0\n\n\n6\n1.0\n12.0\n0.0\n15.0\n4.0\n0.7\n0.0\n\n\n7\n3.0\n12.0\n0.0\n15.0\n3.3\n0.5\n0.0\n\n\n8\n2.0\n12.0\n5.0\n15.0\n4.3\n0.9\n0.0\n\n\n9\n2.0\n12.0\n9.0\n15.0\n4.3\n0.9\n0.0\n\n\n\n\n\nNone\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.482110\n0.418015\n0.832246\n00:09\n\n\n1\n0.349364\n0.333562\n0.852591\n00:07\n\n\n2\n0.322106\n0.323014\n0.854511\n00:07\n\n\n\n\n\n\n\n\nWhy did we get this error?\n\nget_emb_sz??\n\n\nSignature: get_emb_sz(to: 'Tabular | TabularPandas', sz_dict: 'dict' = None) -&gt; 'list'\nSource:   \ndef get_emb_sz(\n    to:Tabular|TabularPandas, \n    sz_dict:dict=None # Dictionary of {'class_name' : size, ...} to override default `emb_sz_rule` \n) -&gt; list: # List of embedding sizes for each category\n    \"Get embedding size for each cat_name in `Tabular` or `TabularPandas`, or populate embedding size manually using sz_dict\"\n    return [_one_emb_sz(to.classes, n, sz_dict) for n in to.cat_names]\nFile:      /opt/homebrew/Caskroom/miniforge/base/envs/fastai/lib/python3.11/site-packages/fastai/tabular/model.py\nType:      function\n\n\n\n\nfrom fastai.tabular.model import _one_emb_sz\n_one_emb_sz??\n\n\nSignature: _one_emb_sz(classes, n, sz_dict=None)\nSource:   \ndef _one_emb_sz(classes, n, sz_dict=None):\n    \"Pick an embedding size for `n` depending on `classes` if not given in `sz_dict`.\"\n    sz_dict = ifnone(sz_dict, {})\n    n_cat = len(classes[n])\n    sz = sz_dict.get(n, int(emb_sz_rule(n_cat)))  # rule of thumb\n    return n_cat,sz\nFile:      /opt/homebrew/Caskroom/miniforge/base/envs/fastai/lib/python3.11/site-packages/fastai/tabular/model.py\nType:      function\n\n\n\nWe see that the error is due to get_emb_sz(dls.train_ds, {} if emb_szs is None else emb_szs) line. The get_emb_sz function tries to return [_one_emb_sz(to.classes, n, sz_dict) for n in to.cat_names]. We get error because our dataloaders has no classes attributes .\nHere, classes attributes is what category do we have in each of our categorical columns. In simple_df below, we would declare aa column as categorical feature, with 3 separate classes [1, 2, 3]. The learner doesn‚Äôt know this because we did not specify to Categorify our categorical column when initializing our dataloaders.\n\nsimple_df = pd.DataFrame({'aa': [1, 2, 3, 1], 'bb':[1.1, 2.2, 3.3, 5.0], 'label':[1, 0, 1, 1]})\nsimple_df\n\n\n\n\n\n\n\n\naa\nbb\nlabel\n\n\n\n\n0\n1\n1.1\n1\n\n\n1\n2\n2.2\n0\n\n\n2\n3\n3.3\n1\n\n\n3\n1\n5.0\n1\n\n\n\n\n\n\n\n\n# no classes attributes\n\nTabularPandas(simple_df, \n              cat_names = ['aa'], \n              cont_names = ['bb'],\n              y_names = ['label'],\n              y_block = CategoryBlock(vocab=simple_df[y_names]), \n              splits = ([0,1,2], [3]),\n             ).dataloaders(bs=64).classes\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\nCell In[32], line 7\n      1 TabularPandas(simple_df, \n      2               cat_names = ['aa'], \n      3               cont_names = ['bb'],\n      4               y_names = ['label'],\n      5               y_block = CategoryBlock(vocab=simple_df[y_names]), \n      6               splits = ([0,1,2], [3]),\n----&gt; 7              ).dataloaders(bs=64).classes\n\nFile /opt/homebrew/Caskroom/miniforge/base/envs/fastai/lib/python3.11/site-packages/fastcore/basics.py:507, in GetAttr.__getattr__(self, k)\n    505 if self._component_attr_filter(k):\n    506     attr = getattr(self,self._default,None)\n--&gt; 507     if attr is not None: return getattr(attr,k)\n    508 raise AttributeError(k)\n\nFile /opt/homebrew/Caskroom/miniforge/base/envs/fastai/lib/python3.11/site-packages/fastcore/basics.py:507, in GetAttr.__getattr__(self, k)\n    505 if self._component_attr_filter(k):\n    506     attr = getattr(self,self._default,None)\n--&gt; 507     if attr is not None: return getattr(attr,k)\n    508 raise AttributeError(k)\n\nFile /opt/homebrew/Caskroom/miniforge/base/envs/fastai/lib/python3.11/site-packages/fastcore/basics.py:507, in GetAttr.__getattr__(self, k)\n    505 if self._component_attr_filter(k):\n    506     attr = getattr(self,self._default,None)\n--&gt; 507     if attr is not None: return getattr(attr,k)\n    508 raise AttributeError(k)\n\nFile /opt/homebrew/Caskroom/miniforge/base/envs/fastai/lib/python3.11/site-packages/fastcore/transform.py:212, in Pipeline.__getattr__(self, k)\n--&gt; 212 def __getattr__(self,k): return gather_attrs(self, k, 'fs')\n\nFile /opt/homebrew/Caskroom/miniforge/base/envs/fastai/lib/python3.11/site-packages/fastcore/transform.py:173, in gather_attrs(o, k, nm)\n    171 att = getattr(o,nm)\n    172 res = [t for t in att.attrgot(k) if t is not None]\n--&gt; 173 if not res: raise AttributeError(k)\n    174 return res[0] if len(res)==1 else L(res)\n\nAttributeError: classes\n\n\n\n\n# now we have classes attributes\n\nTabularPandas(simple_df, \n              cat_names = ['aa'], \n              cont_names = ['bb'],\n              y_names = ['label'],\n              y_block = CategoryBlock(vocab=simple_df[y_names]), \n              splits = ([0,1,2], [3]),\n              procs = [Categorify]\n             ).dataloaders(bs=64).classes\n\n{'aa': ['#na#', 1, 2, 3]}\n\n\nThat‚Äôs all for now, bye!"
  },
  {
    "objectID": "til/200624_numpy_datablock.html",
    "href": "til/200624_numpy_datablock.html",
    "title": "Creating DataBlock from Numpy Array",
    "section": "",
    "text": "Feed list of dict into DataBlock\n\nTLDR; Need to prepare our data into list of dictionaries for each sample, eg L([{x: feature1, y: label1},  {x: feature2, y: label2},  {x: featuren, y: labeln} ]) format and feed the function into get_image param of DataBlock.\n\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom fastai.data.core import Datasets\nfrom fastai.vision.all import *\n\n\nLoad data\n\n# ref: https://www.kaggle.com/code/drkaggle22/digit-recognizer-solution-99-accuracy?scriptVersionId=181451739&cellId=3\nimport struct\n\ndef read_idx(filename):\n    with open(filename, 'rb') as f:\n        zero, data_type, dims= struct.unpack('&gt;HBB', f.read(4))\n        shape = tuple(struct.unpack('&gt;I', f.read(4))[0] for d in range(dims))\n        \n        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n\ndef load_mnist(image_path, label_path):\n    images = read_idx(image_path)\n    labels = read_idx(label_path)\n    return images, labels\n\n\n\ntrain_image_path = '/kaggle/input/mnist-dataset/train-images-idx3-ubyte/train-images-idx3-ubyte'\ntrain_label_path = '/kaggle/input/mnist-dataset/train-labels-idx1-ubyte/train-labels-idx1-ubyte'\ntest_image_path =  '/kaggle/input/mnist-dataset/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte'\ntest_label_path =  '/kaggle/input/mnist-dataset/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte'\n\n\ntrain_images, train_labels = load_mnist(train_image_path, train_label_path)\ntest_images, test_labels = load_mnist(test_image_path, test_label_path)\nprint(f'Train images shape: {train_images.shape}')\nprint(f'Train labels shape: {train_labels.shape}')\nprint(f'Test images shape: {test_images.shape}')\nprint(f'Test labels shape: {test_labels.shape}')\n\nTrain images shape: (60000, 28, 28)\nTrain labels shape: (60000,)\nTest images shape: (10000, 28, 28)\nTest labels shape: (10000,)\n\n\n\nfrom collections import Counter\nprint(Counter(train_labels))\nn_classes = len(Counter(train_labels))\nprint('n_classes:', n_classes)\n\nCounter({1: 6742, 7: 6265, 3: 6131, 2: 5958, 9: 5949, 0: 5923, 6: 5918, 8: 5851, 4: 5842, 5: 5421})\nn_classes: 10\n\n\n\n\ndef tensor_to_labelled_pil_image(tensor: np.ndarray, labels=None) -&gt; list:\n    ''' ref: https://www.kaggle.com/code/pemtaira/digit-recognizer-fastai-v2-2020\n    shape image shape (total sample, height, width) into (total sample, 3, height, width),\n    save into dictionary (x: reshaped img, y: label). Append dictionary to list. return list.\n    '''\n    reshaped = tensor.reshape(-1, 28, 28) #  (total sample, 28, 28) --&gt; (total sample, 28, 28)\n    reshaped = np.stack((reshaped,) *3, axis = 1) # (total sample, 28, 28) --&gt; (total sample, 3, 28, 28)\n    image_arr = []\n    \n    # loop each reshaped images, convert to float tensor, convert to PILImage, save as dictionary, append to list\n    for idx, current_image in enumerate(reshaped):\n        img = torch.tensor(current_image, dtype=torch.float) / 255.\n        img = PILImage(to_image(img))\n        \n        final_data = None\n\n        if (labels is None):\n            final_data = {'x': img, 'y': None}\n        else:\n            final_data = {'x': img, 'y': labels[idx]}\n\n        image_arr.append(final_data)\n\n    return image_arr\n\n\ndef get_image(l:list) -&gt; L:\n    \"\"\"\n    returns list of [{'x': feature tensor, 'y': class label},\n                    {...}, {...} ]\n    L is fastai's implementation of list\n    \"\"\"\n    features = l[0]\n    labels = l[1]\n    all_imgs = tensor_to_labelled_pil_image(features, labels)\n    return L(all_imgs)\n    \n\n\ndef get_y_fromdict(item):\n    \"\"\"get y from each sample dictionary returned from get_image()\"\"\"\n    return item['y']\n\ndef get_x_fromdict(item):\n    \"\"\"get x from each sample dictionary returned from get_image()\"\"\"\n    return item['x']\n\n\n\nInitialize DataBlock\nblocks=(ImageBlock(cls=PILImage), CategoryBlock) &gt; Here we specify that our input data is an image and of class PILImage, our label is categorical\nget_items=get_image &gt; Function where we return list of {x:features, y:label} dictionary for all our samples\nsplitter=RandomSplitter(valid_pct=0.2, seed=42) &gt; Describe how we want to split our data; Here we want to split train and test data into 80-20 split randomly. We specify seed to have reproducible result for each run.\nget_x=get_x_fromdict &gt; Function to extract features from list returned from get_image() function. Note, we could also use lambda function here get_x = (lambda item: item['x']).\nget_y=get_y_fromdict &gt; Function to extract label from list returned from get_image() function. Note, we could also use lambda function here get_y = (lambda item: item['y']).\nNote that if we use lambda function when initializing DataBlock, we might need to use dill library to export our model.\n\nmnist_db = DataBlock(\n    blocks=(ImageBlock(cls=PILImage), CategoryBlock), \n    get_items=get_image, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=get_y_fromdict,#(lambda item: item['y']),\n    get_x=get_x_fromdict #(lambda item: item['x'])\n)\n\n\n# checking featues and labels shapes\nprint(train_images.shape)\nprint(train_labels.shape)\n\n(60000, 28, 28)\n(60000,)\n\n\n\n# stacking both train and test sets' features\n\nprint(train_images.shape)\nprint(test_images.shape)\nnp.vstack([train_images, test_images]).shape\n\n(60000, 28, 28)\n(10000, 28, 28)\n\n\n(70000, 28, 28)\n\n\n\n# stacking both train and test sets\nprint(train_labels.shape)\nprint(test_labels.shape)\nnp.hstack([train_labels, test_labels]).shape\n\n(60000,)\n(10000,)\n\n\n(70000,)\n\n\nThis is how our data is reshaped in tensor_to_labelled_pil_image() function.\n\nfeatures = np.vstack([train_images, test_images])\nfeatures_reshaped = features.reshape(-1, 28, 28)\nfeatures_reshaped_stacked = np.stack((features_reshaped,) *3, axis = 1)\n\nprint('features.shape', features.shape)\nprint('features_reshaped.shape', features_reshaped.shape)\nprint('features_reshaped_stacked.shape', features_reshaped_stacked.shape)\n\nfeatures.shape (70000, 28, 28)\nfeatures_reshaped.shape (70000, 28, 28)\nfeatures_reshaped_stacked.shape (70000, 3, 28, 28)\n\n\n\n\nQuick plot\n\n# ref: https://stackoverflow.com/a/59296746\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(10,10, figsize=(28,28))\nfor i,ax in enumerate(axes.flat):\n    ax.imshow(features_reshaped[i])\n\n\n\n\n\n\n\n\nLoad our source data\n\ndls = mnist_db.dataloaders([np.vstack([train_images, test_images]),\n                            np.hstack([train_labels, test_labels])])\n\n\ndls.show_batch()\n\n\n\n\n\n\n\n\n\n\nTrain model\n\nlearn = vision_learner(dls, resnet18, metrics=[error_rate, accuracy])\nlearn.fine_tune(10)\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00&lt;00:00, 146MB/s]\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\naccuracy\ntime\n\n\n\n\n0\n0.714643\n0.483854\n0.155143\n0.844857\n04:06\n\n\n\n\n\n\n\n\n\n\n    \n      \n      90.00% [9/10 1:10:59&lt;07:53]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\naccuracy\ntime\n\n\n\n\n0\n0.170800\n0.091101\n0.026429\n0.973571\n08:18\n\n\n1\n0.098211\n0.057546\n0.018071\n0.981929\n07:50\n\n\n2\n0.070756\n0.043570\n0.013071\n0.986929\n07:54\n\n\n3\n0.045105\n0.036998\n0.010214\n0.989786\n07:51\n\n\n4\n0.034318\n0.037484\n0.010214\n0.989786\n07:51\n\n\n5\n0.032253\n0.031844\n0.007857\n0.992143\n07:49\n\n\n6\n0.013959\n0.029695\n0.006714\n0.993286\n07:47\n\n\n7\n0.006643\n0.028861\n0.006643\n0.993357\n07:48\n\n\n8\n0.002887\n0.027575\n0.006143\n0.993857\n07:48\n\n\n\n\n\n    \n      \n      22.83% [50/219 00:06&lt;00:21 0.0014]\n    \n    \n\n\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n\n\n\n\nSave model\n\nlearn.export('model2.pkl')\n\nIf we use lambda function when initializing DataBlock, we can use dill to save model. Eg:\n\n\nimport dill\nlearn.export('model2.pkl', pickle_module=dill)"
  },
  {
    "objectID": "til.html",
    "href": "til.html",
    "title": "Today I learn",
    "section": "",
    "text": "Creating DataBlock from Numpy Array\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nIncompleteRead When Downloading From ü™£ AWS S3\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2024\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nFastai Lesson0 Key Insights\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nHow to build üê≥ Docker Image & run Container\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2024\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nTabularPandas AttributeError: classes\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2024\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nConda ImportError when trying to run python file\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2024\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nConda ValueError when trying to open jupyter notebook\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2024\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "til/140524_conda_ImportError.html",
    "href": "til/140524_conda_ImportError.html",
    "title": "Conda ImportError when trying to run python file",
    "section": "",
    "text": "Update typing_extensions when getting ImportError\n\nTraceback (most recent call last):\n...\nfrom typing_extensions import TypeAlias # Python 3.10+ ImportError: cannot import name 'TypeAlias' from 'typing_extensions' (/opt/homebrew/Caskroom/miniforge/base/envs/my_env/lib/python3.10/site-packages/typing_extensions.py)\n\nSteps to reproduce error\n\nOpen terminal\nCreate requirements.txt in current working directory\n\n# requirements.txt\ntyping-extensions ==3.7.4\n\nCreate new environment with python=3.10, install pip, install required libraries from requirements.txt using pip. Here, our environment is called my_env.\n\nconda create -n myenv python=3.10\nconda install pip\npip install -r requirements.txt\n\nCreate a dummy python file called run.py in current working directory\n\n# run.py\nprint('hello world!')\n\nRun run.py from terminal\n\npython run.py\n\n\nHow to fix it\nUpgrade typing_extensions by:\npip install typing_extensions==4.7.1 --upgrade"
  },
  {
    "objectID": "til/130524_conda_ValueError.html",
    "href": "til/130524_conda_ValueError.html",
    "title": "Conda ValueError when trying to open jupyter notebook",
    "section": "",
    "text": "Getting ValueError when forgetting to add conda when trying to activate conda environment\n\nTraceback (most recent call last):\n  File \"/opt/homebrew/Caskroom/miniforge/base/bin/jupyter-notebook\", line 10, in &lt;module&gt;\n    sys.exit(main())\n  File \"/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/jupyter_core/application.py\", line 283, in launch_instance\n    super().launch_instance(argv=argv, **kwargs)\n  File \"/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/traitlets/config/application.py\", line 1074, in launch_instance\n    app.initialize(argv)\n  File \"/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/traitlets/config/application.py\", line 118, in inner\n    return method(app, *args, **kwargs)\n  File \"/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/notebook/notebookapp.py\", line 2171, in initialize\n    self.init_webapp()\n  File \"/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/notebook/notebookapp.py\", line 1779, in init_webapp\n    self.web_app = NotebookWebApplication(\n  File \"/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/notebook/notebookapp.py\", line 178, in __init__\n    settings = self.init_settings(\n  File \"/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/notebook/notebookapp.py\", line 316, in init_settings\n    nbextensions_path=jupyter_app.nbextensions_path,\n  File \"/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/notebook/notebookapp.py\", line 1349, in nbextensions_path\n    from IPython.paths import get_ipython_dir\n  File \"/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/IPython/__init__.py\", line 55, in &lt;module&gt;\n    from .terminal.embed import embed\n  File \"/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/IPython/terminal/embed.py\", line 16, in &lt;module&gt;\n    from IPython.terminal.interactiveshell import TerminalInteractiveShell\n  File \"/opt/homebrew/Caskroom/miniforge/base/lib/python3.10/site-packages/IPython/terminal/interactiveshell.py\", line 31, in &lt;module&gt;\n    from prompt_toolkit.auto_suggest import AutoSuggestFromHistory\nValueError: source code string cannot contain null bytes\n\nSteps to reproduce error\n\nOpen terminal\nActivate your environment, here myenv:\n\n(base) activate myenv\n\nOpen jupyter notebook\n\njupyter notebook\n\n\nHow to fix it\nThe error is because I did not specify conda when trying to activate environment. It should be:\n(base) conda activate myenv\n\n\nWhy we get this error\nWhen we do activate myenv, we are still in our base environment and not myenv. Perhaps some of the libraries required to open jupyter notebook is outdated in our base environment."
  },
  {
    "objectID": "til/200624_incompleteread.html",
    "href": "til/200624_incompleteread.html",
    "title": "IncompleteRead When Downloading From ü™£ AWS S3",
    "section": "",
    "text": "Set keep alive connection to avoid IncompleteRead that is due to connection error\n\n\n\n\nimage.png\n\n\nMy challenge today was figuring out why my code could not run. I got IncompleteRead error when trying to load data from AWS S3. The file is only 400 MB. I tried multiple solutions from github issues to stackoverflow answers. The only workaround that worked for me was setting these in terminal:\n\nsudo sysctl net.inet.tcp.keepintvl=200000\nsudo sysctl net.inet.tcp.keepidle=200000\nsudo sysctl net.inet.tcp.keepinit=200000\nsudo sysctl net.inet.tcp.always_keepalive=1\n\nAnd run again, and again. Now, my code doesn‚Äôt exit out anymore and I still don‚Äôt know why üòÖ\nSome useful references: 1. https://docs.aws.amazon.com/redshift/latest/mgmt/connecting-firewall-guidance.html 2. https://github.com/boto/boto3/issues/2424"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Welcome to my site! I‚Äôm Siti Sarah Amirah, a data scientist passionate about automating processes. I am currently focusing on ML projects to automate subscription services within my organization. Previously, I worked as a fraud investigation analyst within the e-commerce sector. I‚Äôm dedicated to continual learning and self-improvement."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Visualizing Arrays using Excel\n\n\n4 min\n\n\n\n\n\n\nJun 23, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  }
]